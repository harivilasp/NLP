{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Get two texts\n",
    "Remove stop words\n",
    "Map the text to vector spaces\n",
    "Compute cosine(vec1, vec2)\n",
    "Use SciPy\n",
    "Take a call if you should do Stemming or not\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: It  Stem: It\n",
      "Actual: originated  Stem: origin\n",
      "Actual: from  Stem: from\n",
      "Actual: the  Stem: the\n",
      "Actual: idea  Stem: idea\n",
      "Actual: that  Stem: that\n",
      "Actual: there  Stem: there\n",
      "Actual: are  Stem: are\n",
      "Actual: readers  Stem: reader\n",
      "Actual: who  Stem: who\n",
      "Actual: prefer  Stem: prefer\n",
      "Actual: learning  Stem: learn\n",
      "Actual: new  Stem: new\n",
      "Actual: skills  Stem: skill\n",
      "Actual: from  Stem: from\n",
      "Actual: the  Stem: the\n",
      "Actual: comforts  Stem: comfort\n",
      "Actual: of  Stem: of\n",
      "Actual: their  Stem: their\n",
      "Actual: drawing  Stem: draw\n",
      "Actual: rooms  Stem: room\n",
      "Actual: .  Stem: .\n",
      "Actual: Lilies  Stem: lili\n",
      "Actual: are  Stem: are\n",
      "Actual: pretty  Stem: pretti\n",
      "Actual: .  Stem: .\n",
      "\n",
      "\n",
      "After porter stemming we get: It origin from the idea that there are reader who prefer learn new skill from the comfort of their draw room . lili are pretti .\n"
     ]
    }
   ],
   "source": [
    "#We shall calculate the similarity between two documents\n",
    "#These two documents will be produced by stemming the same sentence\n",
    "#The first will be the result of Porter stemming and the second will be a result of Lemmatization\n",
    "\n",
    "#The below program uses the Porter Stemming Algorithm for stemming.\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "document_1 = []\n",
    "word_data = \"It originated from the idea that there are readers who prefer learning new skills from the comforts of their drawing rooms. Lilies are pretty.\"\n",
    "# First Word tokenization\n",
    "nltk_tokens = nltk.word_tokenize(word_data)\n",
    "#Next find the roots of the word\n",
    "for w in nltk_tokens:\n",
    "    print(\"Actual: %s  Stem: %s\"  % (w,porter_stemmer.stem(w)))\n",
    "    document_1.append(porter_stemmer.stem(w))\n",
    "\n",
    "print('\\n')\n",
    "port = ' '.join(word for word in document_1)\n",
    "print(\"After porter stemming we get:\",port)\n",
    "    \n",
    "#When we execute the above code, it produces the following result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: It  Lemma: It\n",
      "Actual: originated  Lemma: originated\n",
      "Actual: from  Lemma: from\n",
      "Actual: the  Lemma: the\n",
      "Actual: idea  Lemma: idea\n",
      "Actual: that  Lemma: that\n",
      "Actual: there  Lemma: there\n",
      "Actual: are  Lemma: are\n",
      "Actual: readers  Lemma: reader\n",
      "Actual: who  Lemma: who\n",
      "Actual: prefer  Lemma: prefer\n",
      "Actual: learning  Lemma: learning\n",
      "Actual: new  Lemma: new\n",
      "Actual: skills  Lemma: skill\n",
      "Actual: from  Lemma: from\n",
      "Actual: the  Lemma: the\n",
      "Actual: comforts  Lemma: comfort\n",
      "Actual: of  Lemma: of\n",
      "Actual: their  Lemma: their\n",
      "Actual: drawing  Lemma: drawing\n",
      "Actual: rooms  Lemma: room\n",
      "Actual: .  Lemma: .\n",
      "Actual: Lilies  Lemma: Lilies\n",
      "Actual: are  Lemma: are\n",
      "Actual: pretty  Lemma: pretty\n",
      "Actual: .  Lemma: .\n",
      "\n",
      "\n",
      "After lemmatization we get: It originated from the idea that there are reader who prefer learning new skill from the comfort of their drawing room . Lilies are pretty .\n"
     ]
    }
   ],
   "source": [
    "#Lemmatization is similar to stemming but it brings context to the words\n",
    "document_2 = []\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "nltk_tokens = nltk.word_tokenize(word_data)\n",
    "for w in nltk_tokens:\n",
    "    print(\"Actual: %s  Lemma: %s\"  % (w,wordnet_lemmatizer.lemmatize(w)))\n",
    "    document_2.append(wordnet_lemmatizer.lemmatize(w))\n",
    "\n",
    "print('\\n')\n",
    "lem = ' '.join(word for word in document_2)\n",
    "print(\"After lemmatization we get:\",lem)\n",
    "        \n",
    "#it produces the following result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine: 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "#Defining and computing cosine similarity\n",
    "import re, math\n",
    "from collections import Counter\n",
    "\n",
    "WORD = re.compile(r'\\w+')\n",
    "\n",
    "def get_cosine(vec1, vec2):\n",
    "     intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "     numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "     sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "     sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "     denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "     if not denominator:\n",
    "        return 0.0\n",
    "     else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "def text_to_vector(text):\n",
    "     words = WORD.findall(text)\n",
    "     return Counter(words)\n",
    "\n",
    "vector1 = text_to_vector(port)\n",
    "vector2 = text_to_vector(lem)\n",
    "\n",
    "cosine = get_cosine(vector1, vector2)\n",
    "\n",
    "print('Cosine:', cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: It  Lancaster: it\n",
      "Actual: originated  Lancaster: origin\n",
      "Actual: from  Lancaster: from\n",
      "Actual: the  Lancaster: the\n",
      "Actual: idea  Lancaster: ide\n",
      "Actual: that  Lancaster: that\n",
      "Actual: there  Lancaster: ther\n",
      "Actual: are  Lancaster: ar\n",
      "Actual: reader  Lancaster: read\n",
      "Actual: who  Lancaster: who\n",
      "Actual: prefer  Lancaster: pref\n",
      "Actual: learning  Lancaster: learn\n",
      "Actual: new  Lancaster: new\n",
      "Actual: skill  Lancaster: skil\n",
      "Actual: from  Lancaster: from\n",
      "Actual: the  Lancaster: the\n",
      "Actual: comfort  Lancaster: comfort\n",
      "Actual: of  Lancaster: of\n",
      "Actual: their  Lancaster: their\n",
      "Actual: drawing  Lancaster: draw\n",
      "Actual: room  Lancaster: room\n",
      "Actual: .  Lancaster: .\n",
      "Actual: Lilies  Lancaster: lily\n",
      "Actual: are  Lancaster: ar\n",
      "Actual: pretty  Lancaster: pretty\n",
      "Actual: .  Lancaster: .\n",
      "\n",
      "\n",
      "After lemmatization and Lancaster stemming we get: it origin from the ide that ther ar read who pref learn new skil from the comfort of their draw room . lily ar pretty .\n"
     ]
    }
   ],
   "source": [
    "#Since cosine similarity is only 0.88, it shows that the documents that resulted were drastically different between \n",
    "#stemming and lemmatization\n",
    "\n",
    "#what if we stem the lemmatized sentence again using Lancaster stemmer instead?\n",
    "\n",
    "document_3 = []\n",
    "from nltk.stem import LancasterStemmer \n",
    "stemmerLan = LancasterStemmer() \n",
    "nltk_tokens = nltk.word_tokenize(lem)\n",
    "for w in nltk_tokens:\n",
    "    print(\"Actual: %s  Lancaster: %s\"  % (w,stemmerLan.stem(w)))\n",
    "    document_3.append(stemmerLan.stem(w))\n",
    "    \n",
    "print('\\n')\n",
    "lanc = ' '.join(word for word in document_3)\n",
    "print(\"After lemmatization and Lancaster stemming we get:\",lanc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since Lemmatization and Lancaster together are not stemming the words correctly, we shall only use lemmatization\n",
    "#we shall now compare president Obama's inaugural speech in English vs Globish after Lemmatization \n",
    "#and stop word removal\n",
    "\n",
    "#Source: http://www.jpn-globish.com/file/obama-speech.pdf\n",
    "\n",
    "#English\n",
    "document_1 = \"\"\"I stand here today humbled by the task before us,\n",
    "grateful for the trust you have bestowed, mindful of the\n",
    "sacrifices borne by our ancestors. I thank President\n",
    "Bush for his service to our nation, as well as the\n",
    "generosity and cooperation he has shown throughout\n",
    "this transition.\n",
    "Forty-four Americans have now taken the presidential\n",
    "oath. The words have been spoken during rising tides\n",
    "of prosperity and the still waters of peace. Yet, every\n",
    "so often the oath is taken amidst gathering clouds and\n",
    "raging storms. At these moments, America has carried\n",
    "on not simply because of the skill or vision of those in\n",
    "high office, but because We the People have remained\n",
    "faithful to the ideals of our forbearers, and true to our\n",
    "founding documents.\n",
    "So it has been. So it must be with this generation of\n",
    "Americans.\n",
    "That we are in the midst of crisis is now well\n",
    "understood. Our nation is at war, against a farreaching\n",
    "network of violence and hatred. Our\n",
    "economy is badly weakened, a consequence of greed\n",
    "and irresponsibility on the part of some, but also our\n",
    "collective failure to make hard choices and prepare\n",
    "the nation for a new age.\n",
    "Homes have been lost; jobs shed; businesses\n",
    "shuttered. Our health care is too costly; our schools\n",
    "fail too many; and each day brings further evidence\n",
    "that the ways we use energy strengthen our\n",
    "adversaries and threaten our planet.\n",
    "These are the indicators of crisis, subject to data and\n",
    "statistics. Less measurable but no less profound is a\n",
    "sapping of confidence across our land – a nagging\n",
    "fear that America’s decline is inevitable, and that the\n",
    "next generation must lower its sights.\n",
    "Today I say to you that the challenges we face are\n",
    "real. They are serious and they are many. They will\n",
    "not be met easily or in a short span of time. But know\n",
    "this, America – they will be met. On this day, we\n",
    "gather because we have chosen hope over fear, unity\n",
    "of purpose over conflict and discord.\n",
    "On this day, we come to proclaim an end to the petty\n",
    "grievances and false promises, the recriminations and\n",
    "worn out dogmas, that for far too long have strangled\n",
    "our politics. \"\"\"\n",
    "\n",
    "#Globish:\n",
    "# \"Jean-Paul Nerrière, the author of Globish presents it as a natural language \n",
    "# as opposed to an artificial or constructed language, claiming that it is a \n",
    "# codification of a reduced set of English patterns as used by non-native speakers of the language.\n",
    "# The name Globish is a portmanteau of \"global\" and \"English\".\" - Wikipedia \n",
    "# (https://en.wikipedia.org/wiki/Globish_(Nerri%C3%A8re))\n",
    "\n",
    "\n",
    "document_2 = \"\"\"I stand here today full of respect for the work before us.\n",
    "I want to thank you for the trust you have given, and I\n",
    "remember the sacrifices made by our ancestors. I thank\n",
    "President Bush for his service to our nation, as well as\n",
    "for the spirit of giving and cooperation he has shown\n",
    "during this change-over.\n",
    "Forty four Americans have now been sworn in as\n",
    "president. The words have been spoken during rising\n",
    "waves of wealth and well-being and the still waters of\n",
    "peace. Yet, every so often, these words of honor are\n",
    "spoken surrounded by gathering clouds and wild storms.\n",
    "At these times, America has carried on not simply\n",
    "because those in high office were skilled or could see\n",
    "into the future. But it has been because We the People\n",
    "have kept believing in the values of our first fathers, and\n",
    "stayed true to the documents that created our country.\n",
    "So it has been. So it must be with this modern-day\n",
    "population of Americans\n",
    "It is well understood now that we are in the middle of a\n",
    "crisis. Our nation is at war, against a far-reaching,\n",
    "organized system of violence and hate. Our economy\n",
    "has been badly weakened. This is the result of extreme\n",
    "desire for great wealth by some people, and failure to\n",
    "act responsibly. But we have all failed to make hard\n",
    "choices and to get the nation ready for a new age.\n",
    "Homes have been lost; jobs given up; businesses have\n",
    "closed. Our health care costs too much; our schools fail\n",
    "too many; and each day brings further proof that the\n",
    "ways we use energy make those against us stronger and\n",
    "threaten our world\n",
    "These are the ways we can measure a crisis. Another\n",
    "problem is just as great, but we cannot measure it as\n",
    "easily. It is the draining of our own belief in America --\n",
    "a fear that America’s fall is surely coming and that\n",
    "future Americans must lower their hopes.\n",
    "Today I say to you that the trials we face are real. They\n",
    "are serious and they are many. They will not be met\n",
    "easily or in a short time. But know this, America -- they\n",
    "will be met. On this day we gather because we have\n",
    "chosen hope over fear. We have chosen united purpose\n",
    "over fighting and over noisy argument.\n",
    "On this day we come to announce an end to narrowminded\n",
    "arguing, to the lies, and to the accusing and\n",
    "worn out teachings that for far too long have killed our\n",
    "politics. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##stopword removal\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "document1 = []\n",
    "document2 = []\n",
    "\n",
    "import re\n",
    "document1 = re.split('; |, |\\*|\\n',document_1)\n",
    "document2 = re.split('; |, |\\*|\\n',document_2)\n",
    "\n",
    "doc1_tok = [nltk.word_tokenize(w) for w in document1]\n",
    "flat_list1 = [item for sublist in doc1_tok for item in sublist]\n",
    "\n",
    "doc2_tok = [nltk.word_tokenize(w) for w in document2]\n",
    "flat_list2 = [item for sublist in doc2_tok for item in sublist]\n",
    "\n",
    "filter1 = [w for w in flat_list1 if not w in stop_words]\n",
    "filter2 = [w for w in flat_list2 if not w in stop_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between English and Globish before Lemmtization:  0.6689692534580189\n",
      "Cosine similarity between English and Globish after Lemmatization: 0.6956343155835307\n"
     ]
    }
   ],
   "source": [
    "##Lemmatization\n",
    "doc_1 = []\n",
    "doc_2 = []\n",
    "filter_1 = []\n",
    "filter_2 = []\n",
    "\n",
    "filter_1 = ' '.join(word for word in filter1)\n",
    "filter_2 = ' '.join(word for word in filter2)\n",
    "\n",
    "for w in filter1:\n",
    "    doc_1.append(stemmerLan.stem(w))\n",
    "\n",
    "lem1 = ' '.join(word for word in doc_1)\n",
    "\n",
    "for w in filter2:\n",
    "    doc_2.append(stemmerLan.stem(w))\n",
    "\n",
    "lem2 = ' '.join(word for word in doc_2)\n",
    "\n",
    "##Cosine Similarity\n",
    "vectora = text_to_vector(filter_1)\n",
    "vectorb = text_to_vector(filter_2)\n",
    "vector1 = text_to_vector(lem1)\n",
    "vector2 = text_to_vector(lem2)\n",
    "\n",
    "cosine1 = get_cosine(vectora, vectorb)\n",
    "cosine2 = get_cosine(vector1, vector2)\n",
    "\n",
    "print('Cosine similarity between English and Globish before Lemmtization: ',cosine1)\n",
    "print('Cosine similarity between English and Globish after Lemmatization:', cosine2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('stand', 'VBP'),\n",
       " ('today', 'NN'),\n",
       " ('humbled', 'VBD'),\n",
       " ('task', 'NN'),\n",
       " ('us', 'PRP'),\n",
       " (',', ','),\n",
       " ('grateful', 'JJ'),\n",
       " ('trust', 'NN'),\n",
       " ('bestowed', 'VBN'),\n",
       " ('mindful', 'JJ'),\n",
       " ('sacrifices', 'NNS'),\n",
       " ('borne', 'JJ'),\n",
       " ('ancestors', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('I', 'PRP'),\n",
       " ('thank', 'VBD'),\n",
       " ('President', 'NNP'),\n",
       " ('Bush', 'NNP'),\n",
       " ('service', 'NN'),\n",
       " ('nation', 'NN'),\n",
       " ('well', 'RB'),\n",
       " ('generosity', 'JJ'),\n",
       " ('cooperation', 'NN'),\n",
       " ('shown', 'VBN'),\n",
       " ('throughout', 'IN'),\n",
       " ('transition', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Forty-four', 'JJ'),\n",
       " ('Americans', 'NNPS'),\n",
       " ('taken', 'VBN'),\n",
       " ('presidential', 'JJ'),\n",
       " ('oath', 'NN'),\n",
       " ('.', '.'),\n",
       " ('The', 'DT'),\n",
       " ('words', 'NNS'),\n",
       " ('spoken', 'VBN'),\n",
       " ('rising', 'VBG'),\n",
       " ('tides', 'NNS'),\n",
       " ('prosperity', 'NN'),\n",
       " ('still', 'RB'),\n",
       " ('waters', 'VBZ'),\n",
       " ('peace', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Yet', 'CC'),\n",
       " ('every', 'DT'),\n",
       " ('often', 'RB'),\n",
       " ('oath', 'VBZ'),\n",
       " ('taken', 'VBN'),\n",
       " ('amidst', 'RP'),\n",
       " ('gathering', 'VBG'),\n",
       " ('clouds', 'NNS'),\n",
       " ('raging', 'VBG'),\n",
       " ('storms', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('At', 'IN'),\n",
       " ('moments', 'NNS'),\n",
       " ('America', 'NNP'),\n",
       " ('carried', 'VBD'),\n",
       " ('simply', 'RB'),\n",
       " ('skill', 'JJ'),\n",
       " ('vision', 'NN'),\n",
       " ('high', 'JJ'),\n",
       " ('office', 'NN'),\n",
       " ('We', 'PRP'),\n",
       " ('People', 'VBP'),\n",
       " ('remained', 'JJ'),\n",
       " ('faithful', 'JJ'),\n",
       " ('ideals', 'NNS'),\n",
       " ('forbearers', 'NNS'),\n",
       " ('true', 'JJ'),\n",
       " ('founding', 'JJ'),\n",
       " ('documents', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('So', 'RB'),\n",
       " ('.', '.'),\n",
       " ('So', 'CC'),\n",
       " ('must', 'MD'),\n",
       " ('generation', 'NN'),\n",
       " ('Americans', 'NNPS'),\n",
       " ('.', '.'),\n",
       " ('That', 'DT'),\n",
       " ('midst', 'NN'),\n",
       " ('crisis', 'NN'),\n",
       " ('well', 'RB'),\n",
       " ('understood', 'RB'),\n",
       " ('.', '.'),\n",
       " ('Our', 'PRP$'),\n",
       " ('nation', 'NN'),\n",
       " ('war', 'NN'),\n",
       " ('farreaching', 'NN'),\n",
       " ('network', 'NN'),\n",
       " ('violence', 'NN'),\n",
       " ('hatred', 'VBD'),\n",
       " ('.', '.'),\n",
       " ('Our', 'PRP$'),\n",
       " ('economy', 'NN'),\n",
       " ('badly', 'RB'),\n",
       " ('weakened', 'VBD'),\n",
       " ('consequence', 'NN'),\n",
       " ('greed', 'NN'),\n",
       " ('irresponsibility', 'NN'),\n",
       " ('part', 'NN'),\n",
       " ('also', 'RB'),\n",
       " ('collective', 'JJ'),\n",
       " ('failure', 'NN'),\n",
       " ('make', 'VBP'),\n",
       " ('hard', 'JJ'),\n",
       " ('choices', 'NNS'),\n",
       " ('prepare', 'VB'),\n",
       " ('nation', 'NN'),\n",
       " ('new', 'JJ'),\n",
       " ('age', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Homes', 'VBZ'),\n",
       " ('lost', 'VBN'),\n",
       " ('jobs', 'NNS'),\n",
       " ('shed', 'VBN'),\n",
       " ('businesses', 'NNS'),\n",
       " ('shuttered', 'VBD'),\n",
       " ('.', '.'),\n",
       " ('Our', 'PRP$'),\n",
       " ('health', 'NN'),\n",
       " ('care', 'NN'),\n",
       " ('costly', 'JJ'),\n",
       " ('schools', 'NNS'),\n",
       " ('fail', 'VBP'),\n",
       " ('many', 'JJ'),\n",
       " ('day', 'NN'),\n",
       " ('brings', 'VBZ'),\n",
       " ('evidence', 'NN'),\n",
       " ('ways', 'NNS'),\n",
       " ('use', 'VBP'),\n",
       " ('energy', 'NN'),\n",
       " ('strengthen', 'NN'),\n",
       " ('adversaries', 'NNS'),\n",
       " ('threaten', 'VBP'),\n",
       " ('planet', 'NN'),\n",
       " ('.', '.'),\n",
       " ('These', 'DT'),\n",
       " ('indicators', 'NNS'),\n",
       " ('crisis', 'NN'),\n",
       " ('subject', 'JJ'),\n",
       " ('data', 'NN'),\n",
       " ('statistics', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('Less', 'RBR'),\n",
       " ('measurable', 'JJ'),\n",
       " ('less', 'RBR'),\n",
       " ('profound', 'JJ'),\n",
       " ('sapping', 'VBG'),\n",
       " ('confidence', 'NN'),\n",
       " ('across', 'IN'),\n",
       " ('land', 'NN'),\n",
       " ('–', 'NN'),\n",
       " ('nagging', 'VBG'),\n",
       " ('fear', 'NN'),\n",
       " ('America', 'NNP'),\n",
       " ('’', 'NNP'),\n",
       " ('decline', 'NN'),\n",
       " ('inevitable', 'JJ'),\n",
       " ('next', 'JJ'),\n",
       " ('generation', 'NN'),\n",
       " ('must', 'MD'),\n",
       " ('lower', 'VB'),\n",
       " ('sights', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('Today', 'NN'),\n",
       " ('I', 'PRP'),\n",
       " ('say', 'VBP'),\n",
       " ('challenges', 'NNS'),\n",
       " ('face', 'VBP'),\n",
       " ('real', 'JJ'),\n",
       " ('.', '.'),\n",
       " ('They', 'PRP'),\n",
       " ('serious', 'JJ'),\n",
       " ('many', 'JJ'),\n",
       " ('.', '.'),\n",
       " ('They', 'PRP'),\n",
       " ('met', 'VBD'),\n",
       " ('easily', 'RB'),\n",
       " ('short', 'JJ'),\n",
       " ('span', 'NN'),\n",
       " ('time', 'NN'),\n",
       " ('.', '.'),\n",
       " ('But', 'CC'),\n",
       " ('know', 'VBP'),\n",
       " ('America', 'NNP'),\n",
       " ('–', 'NNP'),\n",
       " ('met', 'VBD'),\n",
       " ('.', '.'),\n",
       " ('On', 'IN'),\n",
       " ('day', 'NN'),\n",
       " ('gather', 'CC'),\n",
       " ('chosen', 'VBN'),\n",
       " ('hope', 'VBP'),\n",
       " ('fear', 'JJ'),\n",
       " ('unity', 'NN'),\n",
       " ('purpose', 'VBP'),\n",
       " ('conflict', 'NN'),\n",
       " ('discord', 'NN'),\n",
       " ('.', '.'),\n",
       " ('On', 'IN'),\n",
       " ('day', 'NN'),\n",
       " ('come', 'VB'),\n",
       " ('proclaim', 'JJ'),\n",
       " ('end', 'NN'),\n",
       " ('petty', 'NN'),\n",
       " ('grievances', 'NNS'),\n",
       " ('false', 'JJ'),\n",
       " ('promises', 'NNS'),\n",
       " ('recriminations', 'NNS'),\n",
       " ('worn', 'VBP'),\n",
       " ('dogmas', 'RB'),\n",
       " ('far', 'RB'),\n",
       " ('long', 'RB'),\n",
       " ('strangled', 'JJ'),\n",
       " ('politics', 'NNS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tagging the words\n",
    "nltk.pos_tag(filter1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Inference:\n",
    "\n",
    "Lemmatizing has the following advantages over stemming:\n",
    "1. It preserves the context of the tense, for e.g., originated lemmatizes to originated. The tense is still a past\n",
    "   participle. This means it preserves the syntatical class of the word.\n",
    "2. It takes into consideration the morphological analysis of the words. To do so, it is necessary to have detailed \n",
    "   dictionaries which the algorithm can look through to link the form back to its lemma. Another important difference \n",
    "   to highlight is that a lemma is the base form of all its inflectional forms, whereas a stem isn’t. \n",
    "   This is why regular dictionaries are lists of lemmas, not stems.\n",
    "\n",
    "The cosine similiarity after lemmatization increased by 0.03, which is the magnitude of 1. Despite a very small increase\n",
    "in a numerical sense, intuitively this 0.03 increase is significant. This is a 4% increase. This provides evidence that by \n",
    "preserving the lemma of the word, we can identify similarity between two web pages. This could prove to be the first stage \n",
    "in developing a plagiarism detection tools for academic purposes or for search engines to eliminate duplicate pages from their \n",
    "search results. Although it could be argued that plagirism tools would prefer to use a stemmer such as Porter. Such stemmers \n",
    "provide supurious results such as stemming 'Lilies' to 'Lili'. This would likely result in overfitting the two documents.\n",
    "Another extension would be the use of min-hash rather than cosine similarity to identify how identical two documents.\n",
    "\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
